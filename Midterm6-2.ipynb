{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T10:53:46.178818Z","iopub.execute_input":"2024-03-20T10:53:46.179253Z","iopub.status.idle":"2024-03-20T10:53:46.186053Z","shell.execute_reply.started":"2024-03-20T10:53:46.179218Z","shell.execute_reply":"2024-03-20T10:53:46.185149Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier, export_text\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\ntree = DecisionTreeClassifier(max_depth=5)\ntree.fit(X_train, y_train)\ntree_rules = export_text(tree, feature_names=list(data.feature_names))\n\nprint(\"Conditional Clause Tree:\" + \"\\n\")\nprint(tree_rules)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.187773Z","iopub.execute_input":"2024-03-20T10:53:46.189030Z","iopub.status.idle":"2024-03-20T10:53:46.228672Z","shell.execute_reply.started":"2024-03-20T10:53:46.188993Z","shell.execute_reply":"2024-03-20T10:53:46.227693Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Conditional Clause Tree:\n\n|--- worst perimeter <= 105.95\n|   |--- worst concave points <= 0.16\n|   |   |--- worst concave points <= 0.14\n|   |   |   |--- area error <= 48.98\n|   |   |   |   |--- smoothness error <= 0.00\n|   |   |   |   |   |--- class: 1\n|   |   |   |   |--- smoothness error >  0.00\n|   |   |   |   |   |--- class: 1\n|   |   |   |--- area error >  48.98\n|   |   |   |   |--- compactness error <= 0.06\n|   |   |   |   |   |--- class: 0\n|   |   |   |   |--- compactness error >  0.06\n|   |   |   |   |   |--- class: 1\n|   |   |--- worst concave points >  0.14\n|   |   |   |--- mean texture <= 20.78\n|   |   |   |   |--- class: 1\n|   |   |   |--- mean texture >  20.78\n|   |   |   |   |--- class: 0\n|   |--- worst concave points >  0.16\n|   |   |--- compactness error <= 0.08\n|   |   |   |--- class: 0\n|   |   |--- compactness error >  0.08\n|   |   |   |--- class: 1\n|--- worst perimeter >  105.95\n|   |--- worst texture <= 20.65\n|   |   |--- worst radius <= 17.64\n|   |   |   |--- class: 1\n|   |   |--- worst radius >  17.64\n|   |   |   |--- smoothness error <= 0.00\n|   |   |   |   |--- class: 1\n|   |   |   |--- smoothness error >  0.00\n|   |   |   |   |--- class: 0\n|   |--- worst texture >  20.65\n|   |   |--- mean concave points <= 0.03\n|   |   |   |--- mean radius <= 14.93\n|   |   |   |   |--- class: 1\n|   |   |   |--- mean radius >  14.93\n|   |   |   |   |--- mean concavity <= 0.05\n|   |   |   |   |   |--- class: 0\n|   |   |   |   |--- mean concavity >  0.05\n|   |   |   |   |   |--- class: 1\n|   |   |--- mean concave points >  0.03\n|   |   |   |--- fractal dimension error <= 0.01\n|   |   |   |   |--- mean concave points <= 0.04\n|   |   |   |   |   |--- class: 0\n|   |   |   |   |--- mean concave points >  0.04\n|   |   |   |   |   |--- class: 0\n|   |   |   |--- fractal dimension error >  0.01\n|   |   |   |   |--- class: 1\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Accuracy: \" + str(sum(tree.predict(X_test) == y_test) / len(y_test)))\nprint(\"Number of resulting clauses: \" + str(tree_rules.count(\"class\")))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.229799Z","iopub.execute_input":"2024-03-20T10:53:46.231035Z","iopub.status.idle":"2024-03-20T10:53:46.238140Z","shell.execute_reply.started":"2024-03-20T10:53:46.230997Z","shell.execute_reply":"2024-03-20T10:53:46.236870Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Accuracy: 0.9298245614035088\nNumber of resulting clauses: 17\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this first example, I used a decision tree to create 17 conditional clauses (max depth of 5). The data was from a binary sklearn breast cancer diagnosis dataset. Held out 20% of the data for testing accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Part b","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\niris = load_iris()\n# Selecting two out of three species to classify\nbinary_indices = iris.target != 2  # Exclude one class\nX, y = iris.data[binary_indices], iris.target[binary_indices]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\ntree = DecisionTreeClassifier(max_depth=4)\ntree.fit(X_train, y_train)\ntree_rules = export_text(tree, feature_names=list(iris.feature_names))\n\nprint(\"Conditional Clause Tree:\" + \"\\n\")\nprint(tree_rules)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.241113Z","iopub.execute_input":"2024-03-20T10:53:46.241594Z","iopub.status.idle":"2024-03-20T10:53:46.255829Z","shell.execute_reply.started":"2024-03-20T10:53:46.241552Z","shell.execute_reply":"2024-03-20T10:53:46.254673Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Conditional Clause Tree:\n\n|--- petal width (cm) <= 0.80\n|   |--- class: 0\n|--- petal width (cm) >  0.80\n|   |--- class: 1\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Accuracy: \" + str(sum(tree.predict(X_test) == y_test) / len(y_test)))\nprint(\"Number of resulting clauses: \" + str(tree_rules.count(\"class\")))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.257978Z","iopub.execute_input":"2024-03-20T10:53:46.259009Z","iopub.status.idle":"2024-03-20T10:53:46.269995Z","shell.execute_reply.started":"2024-03-20T10:53:46.258967Z","shell.execute_reply":"2024-03-20T10:53:46.268856Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Accuracy: 1.0\nNumber of resulting clauses: 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this example, I used a decision tree on the sklearn iris dataset which classifies three types of plants using four features. To make this a binary dataset, I removed all data where the class was not 0 or 1. Although there are four features, it seems like a single feature (petal_width) was enough to create a perfect split between classes 0 and 1. Thus, we only see 2 resulting clauses here.","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:27:12.614050Z","iopub.execute_input":"2024-03-20T10:27:12.614508Z","iopub.status.idle":"2024-03-20T10:27:12.627139Z","shell.execute_reply.started":"2024-03-20T10:27:12.614476Z","shell.execute_reply":"2024-03-20T10:27:12.625817Z"}}},{"cell_type":"code","source":"from sklearn.datasets import load_digits\ndigits = load_digits()\n# Convert to binary by classifying 0 vs any other digits\nX, y = digits.data, (digits.target == 0).astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\ntree = DecisionTreeClassifier(max_depth=5)\ntree.fit(X_train, y_train)\ntree_rules = export_text(tree, feature_names=list(digits.feature_names))\n\nprint(\"Conditional Clause Tree:\" + \"\\n\")\nprint(tree_rules)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.272397Z","iopub.execute_input":"2024-03-20T10:53:46.273394Z","iopub.status.idle":"2024-03-20T10:53:46.309800Z","shell.execute_reply.started":"2024-03-20T10:53:46.273321Z","shell.execute_reply":"2024-03-20T10:53:46.308585Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Conditional Clause Tree:\n\n|--- pixel_4_4 <= 0.50\n|   |--- pixel_3_4 <= 2.50\n|   |   |--- pixel_2_5 <= 1.00\n|   |   |   |--- pixel_1_6 <= 5.50\n|   |   |   |   |--- class: 0\n|   |   |   |--- pixel_1_6 >  5.50\n|   |   |   |   |--- class: 1\n|   |   |--- pixel_2_5 >  1.00\n|   |   |   |--- pixel_2_2 <= 2.50\n|   |   |   |   |--- pixel_4_5 <= 12.50\n|   |   |   |   |   |--- class: 1\n|   |   |   |   |--- pixel_4_5 >  12.50\n|   |   |   |   |   |--- class: 0\n|   |   |   |--- pixel_2_2 >  2.50\n|   |   |   |   |--- class: 1\n|   |--- pixel_3_4 >  2.50\n|   |   |--- pixel_5_2 <= 13.50\n|   |   |   |--- class: 0\n|   |   |--- pixel_5_2 >  13.50\n|   |   |   |--- pixel_3_3 <= 13.50\n|   |   |   |   |--- class: 1\n|   |   |   |--- pixel_3_3 >  13.50\n|   |   |   |   |--- class: 0\n|--- pixel_4_4 >  0.50\n|   |--- pixel_4_4 <= 1.50\n|   |   |--- pixel_3_4 <= 0.50\n|   |   |   |--- pixel_0_5 <= 7.00\n|   |   |   |   |--- class: 1\n|   |   |   |--- pixel_0_5 >  7.00\n|   |   |   |   |--- class: 0\n|   |   |--- pixel_3_4 >  0.50\n|   |   |   |--- class: 0\n|   |--- pixel_4_4 >  1.50\n|   |   |--- pixel_4_4 <= 3.50\n|   |   |   |--- pixel_2_6 <= 6.50\n|   |   |   |   |--- class: 0\n|   |   |   |--- pixel_2_6 >  6.50\n|   |   |   |   |--- pixel_0_6 <= 5.00\n|   |   |   |   |   |--- class: 1\n|   |   |   |   |--- pixel_0_6 >  5.00\n|   |   |   |   |   |--- class: 0\n|   |   |--- pixel_4_4 >  3.50\n|   |   |   |--- class: 0\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Accuracy: \" + str(sum(tree.predict(X_test) == y_test) / len(y_test)))\nprint(\"Number of resulting clauses: \" + str(tree_rules.count(\"class\")))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.311705Z","iopub.execute_input":"2024-03-20T10:53:46.312433Z","iopub.status.idle":"2024-03-20T10:53:46.319740Z","shell.execute_reply.started":"2024-03-20T10:53:46.312388Z","shell.execute_reply":"2024-03-20T10:53:46.318614Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Accuracy: 0.9944444444444445\nNumber of resulting clauses: 15\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this example, I used a dataset of image pixels in pictures of numbers, paired with a digit label. Once again to make the task binary, I converted the dataset into classifying zeros versus non-zeros. The number of resulting clauses was 15.","metadata":{}},{"cell_type":"markdown","source":"## Part c","metadata":{}},{"cell_type":"code","source":"for num_features in range(2, 50, 4):\n    num_samples = 100\n    X_random = np.random.rand(num_samples, num_features)\n    y_random = np.random.randint(2, size=num_samples)\n    tree_random = DecisionTreeClassifier(max_depth=100)\n    tree_random.fit(X_random, y_random)\n    tree_rules_random = export_text(tree_random)\n    print(\"# features: \" + str(num_features) + \", \" + \"# clauses: \" + str(tree_rules_random.count(\"class\")))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:53:46.321243Z","iopub.execute_input":"2024-03-20T10:53:46.321632Z","iopub.status.idle":"2024-03-20T10:53:46.364457Z","shell.execute_reply.started":"2024-03-20T10:53:46.321603Z","shell.execute_reply":"2024-03-20T10:53:46.363325Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"# features: 2, # clauses: 36\n# features: 6, # clauses: 28\n# features: 10, # clauses: 23\n# features: 14, # clauses: 20\n# features: 18, # clauses: 19\n# features: 22, # clauses: 17\n# features: 26, # clauses: 18\n# features: 30, # clauses: 17\n# features: 34, # clauses: 16\n# features: 38, # clauses: 16\n# features: 42, # clauses: 13\n# features: 46, # clauses: 15\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here I generated a random dataset of 100 samples with varying numbers of features. The output of the previous cell shows tests with different numbers of features and the number of resulting clauses. It demonstrates that the number of resulting clauses needed to best fit the data generally decreases as the number of features increases. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}